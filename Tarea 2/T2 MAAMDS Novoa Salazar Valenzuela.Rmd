---
title: "T2MAAMS"
output: html_document
date: "2022-10-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tarea 2: Métodos de Aprendizaje de Máquinas en Data Science

Sebastián Novoa, Nelson Salazar y Nicolás Valenzuela

```{r}
rm(list=ls())
pacman::p_load(tidyverse, tidymodels, kknn, caret)
set.seed(23)

tidymodels_prefer()

data<-read.csv("dataTrain.csv")
test<-read.csv("dataEval.csv")

summary(data$diameter)
```


```{r}

## Viendo nulos de Eval: ####

#Tuplas con nulos:

Tuplas_na<-sum(apply(X=is.na(test),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos

#columnas con nulos:
Column_na<-sum(apply(X=is.na(test),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(test),MARGIN=2,FUN =sum)>0
a<-apply(X=is.na(test),MARGIN=2,FUN =sum)[valores_col]
a
Col_Eval_nulos<-names(a[a>37500])
Col_Eval_nulos

## Viendo nulos de Train: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(data),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos

#columnas con nulos:
Column_na<-sum(apply(X=is.na(data),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(data),MARGIN=2,FUN =sum)>0
b<-apply(X=is.na(data),MARGIN=2,FUN =sum)[valores_col]
b
Col_Train_nulos<-names(b[b>99500])
Col_Train_nulos
#Eliminamos los indices:
data$X<-NULL
data$index<-NULL
data$full_name<-NULL
#Elimnamos las columnas con mayor cantidad de nulos.
data$G<-NULL
data$GM<-NULL
data$BV<-NULL
data$UB<-NULL
data$IR<-NULL
#Borramos variables categoricas que no afectan a nuestro problema.
apply(X=is.na(data),MARGIN=2,FUN =sum)
data$neo<-NULL
data$pha<-NULL
data$extent<-NULL
data$spec_B<-NULL
data$spec_T<-NULL

#Siguen habiendo 99970 tuplas con nulos.

```


```{r}
# Analisis Descriptivo ####
str(data)
nombres<-names(select_if(data, is.numeric))
nombres

medias<-c(mean(data$a,na.rm = TRUE),mean(data$e,na.rm = TRUE),mean(data$i,na.rm = TRUE)
          ,mean(data$om,na.rm = TRUE),mean(data$w,na.rm = TRUE),mean(data$q,na.rm = TRUE)
          ,mean(data$ad,na.rm = TRUE),mean(data$per_y,na.rm = TRUE),mean(data$data_arc,na.rm = TRUE)
          ,mean(data$condition_code,na.rm = TRUE),mean(data$n_obs_used,na.rm = TRUE)
          ,mean(data$H,na.rm = TRUE),mean(data$diameter,na.rm = TRUE),mean(data$albedo,na.rm = TRUE)
          ,mean(data$rot_per,na.rm = TRUE),mean(data$moid,na.rm = TRUE))
#guardamos data con nulos:
data2=data
#Remplazar los nulos con la media.
for (i in 1:16){
  variable=nombres[i]
  data[[variable]]<- round(data[[variable]] %>%
                             replace(is.na(.),medias[i]), digits = 6)
}

#columnas con su respectivo numero de nulos
apply(X=is.na(data),MARGIN=2,FUN =sum)
#Correlaciones

muestra<-select_if(data,is.numeric)
#Menor a mayor considerando negativos:
cor(muestra, method="pearson")[13,][order(cor(muestra, method="pearson")[15,])]
#Menor a mayor en absoluto:
sort(abs(cor(muestra, method="pearson")[13,]))
sort(abs(cor(muestra, method="spearman")[13,]))

#Borramos data con correlacion bajo 0.2 en pearson y 0.3 en spearmann
data$w<-NULL
data$rot_per<-NULL
data$om<-NULL
data$e<-NULL
data$i<-NULL
data$albedo<-NULL
data$condition_code<-NULL
muestra<-select_if(data,is.numeric)
sort(abs(cor(muestra, method="pearson")[8,]))
sort(abs(cor(muestra, method="spearman")[8,]))
#Data hasta ahora considerando lo realizado con anterioridad.
nombres<-names(select_if(data, is.numeric))
nombres
for (i in 1:9){
  variable=nombres[i]
  hist(data[[variable]],main=nombres[i],col="red")
}

```

Ahora eliminamos outliers. Para entrenar con un número variado de diámetros -- y considerando que la media de los diámetros en el dataset original es de 5.476 --, es que sacaremos todos aquellos valores mayores de 20.

```{r}

data<-data[data$diameter < 20, ]
for (i in 1:9){
  variable=nombres[i]
  hist(data[[variable]],main=nombres[i],col="blue")
}

summary(data$diameter)
#nulos por media, elimine categoricas y elimine correlaciones segun pearson y spearmann.

```

Ahora se aplica el mismo proceso a los datos a evaluar.

```{r}
##Realizamos mismo tratamiento para test.####
#Eliminamos variables.
test$X<-NULL
test$index<-NULL
test$full_name<-NULL
test$G<-NULL
test$GM<-NULL
test$BV<-NULL
test$UB<-NULL
test$IR<-NULL
test$neo<-NULL
test$pha<-NULL
test$extent<-NULL
test$spec_B<-NULL
test$spec_T<-NULL
test$w<-NULL
test$rot_per<-NULL
test$om<-NULL
test$e<-NULL
test$i<-NULL
test$albedo<-NULL
test$condition_code<-NULL

nombres<-names(select_if(test, is.numeric))
nombres

medias<-c(mean(test$a,na.rm = TRUE),
          mean(test$q,na.rm = TRUE),
          mean(test$ad,na.rm = TRUE),
          mean(test$per_y,na.rm = TRUE),
          mean(test$data_arc,na.rm = TRUE),
          mean(test$n_obs_used,na.rm = TRUE),
          mean(test$H,na.rm = TRUE),
          mean(test$moid,na.rm = TRUE)
)
#Remplazar los nulos con la media.
for (i in 1:8){
  variable=nombres[i]
  test[[variable]]<- round(test[[variable]] %>%
                             replace(is.na(.),medias[i]), digits = 6)
}
```


Viendo cuantos K vecinos podrían ser necesarios:

```{r}
mod_test <-
  nearest_neighbor(neighbors = tune(), weight_func = "triangular") %>%
  set_mode("regression") %>%
  set_engine("kknn")

knn_param <- extract_parameter_set_dials(mod_test)
knn_param %>% extract_parameter_dials("neighbors")

```

Luego, como la función tune() recomienda un K de entre 1 y 15, evaluamos cada uno de estos, y buscamos el que tiene menor ECM:

```{r}
ecm_vec = c() 

for(i in 1:15){
  
  print(paste("K: ", i))
  
  x_train = data[,-c(8)]
  y_train = data[8]
  
  # Normalizando
  
  process1 <- preProcess(as.data.frame(x_train), method=c("range"))
  x_train_norm <- predict(process1, as.data.frame(x_train))
  
  x_train_norm['diameter'] = y_train['diameter']
  
  # Construcción del modelo
  
  knn_reg_spec <-
    nearest_neighbor(neighbors = i, weight_func = "triangular") %>%
    set_mode("regression") %>%
    set_engine("kknn")
  
  knn_reg_fit <- knn_reg_spec %>% fit(diameter ~ ., data = x_train_norm)
  
  ecm_vec = append(ecm_vec, knn_reg_fit$fit$MEAN.SQU)
  
}

ecm_vec 

```


```{r}
k = 1:15
plot(k, ecm_vec, main = 'Evolución del ECM con respecto a la cantidad de vecinos')
```

Del resultado anterior, podemos ver que con K = 15 se obtiene el menor ECM, así que rearmamos ese modelo, y evaluamos con los asteroides faltantes:

```{r}
process2 <- preProcess(as.data.frame(test), method=c("range"))
dataEval_norm <- predict(process2, as.data.frame(test))

knn_reg_spec2 <-
  nearest_neighbor(neighbors = 15, weight_func = "triangular") %>%
  set_mode("regression") %>%
  set_engine("kknn")

knn_reg_spec2

knn_reg_fit2 <- knn_reg_spec2 %>% fit(diameter ~ ., data = x_train_norm)
knn_reg_fit2

pred2 = predict(knn_reg_fit2, dataEval_norm)

print('MSE modelo:')
print(knn_reg_fit2$fit$MEAN.SQU)

summary(pred2)

write.csv(pred2,"pred_meteo.csv", row.names = FALSE)
```

Podemos ver que nuestro modelo predice, en promedio, asteroides con un diámetro 4.67. El valor mínimo y máximo son de 0.06 y 17.42, respectivamente; mientras que el ECM de nuestro modelo es de 2.654326. Es decir, el RMSE (el error en la unidad de medida del diámetro del asteroide) es de 1.62921, lo cual es bastante bajo.
Así, el modelo parece ser adecuado ante lo solicitado. Entrega resultados bastante acordes comparados con el dataset original, con valores contenidos entre los 0 y 10 unidades de diámetro.

