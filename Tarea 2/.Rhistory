sort(abs(cor(muestra, method="pearson")[8,]))
sort(abs(cor(muestra, method="spearman")[8,]))
write.csv(Data, file="data_nulos_media.csv")
write.csv(data, file="data_nulos_media.csv")
summary(data["diameter"][algo])
hist(x=data$diameter[algo])
algo<-data$diameter[data["diameter"]<100]
print(length(algo))
summary(data["diameter"][algo])
hist(x=data$diameter[algo])
hist(x=algo)
algo<-data$diameter[data["diameter"]<40]
hist(x=algo)
algo<-data$diameter[data["diameter"]<30]
print(length(algo))
summary(data["diameter"][algo])
hist(x=algo)
algo<-data$diameter[data["diameter"]<20]
hist(x=algo)
print(length(algo))
summary(algo)
summary(data$diameter)
pacman::p_load(psych,ggplot2,tidyverse,proxy,dplyr,rlang,purrr)
data<-read.csv("dataTrain.csv")
test<-read.csv("dataEval.csv")
## Viendo nulos de Eval: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(test),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos
#columnas con nulos:
Column_na<-sum(apply(X=is.na(test),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(test),MARGIN=2,FUN =sum)>0
a<-apply(X=is.na(test),MARGIN=2,FUN =sum)[valores_col]
a
Col_Eval_nulos<-names(a[a>37500])
Col_Eval_nulos
## Viendo nulos de Train: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(data),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos
#columnas con nulos:
Column_na<-sum(apply(X=is.na(data),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(data),MARGIN=2,FUN =sum)>0
b<-apply(X=is.na(data),MARGIN=2,FUN =sum)[valores_col]
b
Col_Train_nulos<-names(b[b>99500])
Col_Train_nulos
#Eliminamos los indices:
data$X<-NULL
data$index<-NULL
data$full_name<-NULL
#Elimnamos las columnas con mayor cantidad de nulos.
data$G<-NULL
data$GM<-NULL
data$BV<-NULL
data$UB<-NULL
data$IR<-NULL
#Borramos variables categoricas que no afectan a nuestro problema.
apply(X=is.na(data),MARGIN=2,FUN =sum)
data$neo<-NULL
data$pha<-NULL
data$extent<-NULL
data$spec_B<-NULL
data$spec_T<-NULL
#Siguen habiendo 99970 tuplas con nulos.
# Analisis Descriptivo ####
str(data)
nombres<-names(select_if(data, is.numeric))
nombres
medias<-c(mean(data$a,na.rm = TRUE),mean(data$e,na.rm = TRUE),mean(data$i,na.rm = TRUE)
,mean(data$om,na.rm = TRUE),mean(data$w,na.rm = TRUE),mean(data$q,na.rm = TRUE)
,mean(data$ad,na.rm = TRUE),mean(data$per_y,na.rm = TRUE),mean(data$data_arc,na.rm = TRUE)
,mean(data$condition_code,na.rm = TRUE),mean(data$n_obs_used,na.rm = TRUE)
,mean(data$H,na.rm = TRUE),mean(data$diameter,na.rm = TRUE),mean(data$albedo,na.rm = TRUE)
,mean(data$rot_per,na.rm = TRUE),mean(data$moid,na.rm = TRUE))
#guardamos data con nulos:
data2=data
#Remplazar los nulos con la media.
for (i in 1:16){
variable=nombres[i]
data[[variable]]<- round(data[[variable]] %>%
replace(is.na(.),medias[i]), digits = 6)
}
#columnas con su respectivo numero de nulos
apply(X=is.na(data),MARGIN=2,FUN =sum)
#Correlaciones
muestra<-select_if(data,is.numeric)
#Menor a mayor considerando negativos:
cor(muestra, method="pearson")[13,][order(cor(muestra, method="pearson")[15,])]
#Menor a mayor en absoluto:
sort(abs(cor(muestra, method="pearson")[13,]))
sort(abs(cor(muestra, method="spearman")[13,]))
#Borramos data con correlacion bajo 0.2 en pearson y 0.3 en spearmann
data$w<-NULL
data$rot_per<-NULL
data$om<-NULL
data$e<-NULL
data$i<-NULL
data$albedo<-NULL
data$condition_code<-NULL
muestra<-select_if(data,is.numeric)
sort(abs(cor(muestra, method="pearson")[8,]))
sort(abs(cor(muestra, method="spearman")[8,]))
#Data hasta ahora considerando lo realizado con anterioridad.
nombres<-names(select_if(data, is.numeric))
nombres
for (i in 1:16){
variable=nombres[i]
hist(data[[variable]],main=nombres[i],col="red")
}
pacman::p_load(psych,ggplot2,tidyverse,proxy,dplyr,rlang,purrr)
data<-read.csv("dataTrain.csv")
test<-read.csv("dataEval.csv")
## Viendo nulos de Eval: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(test),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos
#columnas con nulos:
Column_na<-sum(apply(X=is.na(test),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(test),MARGIN=2,FUN =sum)>0
a<-apply(X=is.na(test),MARGIN=2,FUN =sum)[valores_col]
a
Col_Eval_nulos<-names(a[a>37500])
Col_Eval_nulos
## Viendo nulos de Train: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(data),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos
#columnas con nulos:
Column_na<-sum(apply(X=is.na(data),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(data),MARGIN=2,FUN =sum)>0
b<-apply(X=is.na(data),MARGIN=2,FUN =sum)[valores_col]
b
Col_Train_nulos<-names(b[b>99500])
Col_Train_nulos
#Eliminamos los indices:
data$X<-NULL
data$index<-NULL
data$full_name<-NULL
#Elimnamos las columnas con mayor cantidad de nulos.
data$G<-NULL
data$GM<-NULL
data$BV<-NULL
data$UB<-NULL
data$IR<-NULL
#Borramos variables categoricas que no afectan a nuestro problema.
apply(X=is.na(data),MARGIN=2,FUN =sum)
data$neo<-NULL
data$pha<-NULL
data$extent<-NULL
data$spec_B<-NULL
data$spec_T<-NULL
#Siguen habiendo 99970 tuplas con nulos.
# Analisis Descriptivo ####
str(data)
nombres<-names(select_if(data, is.numeric))
nombres
medias<-c(mean(data$a,na.rm = TRUE),mean(data$e,na.rm = TRUE),mean(data$i,na.rm = TRUE)
,mean(data$om,na.rm = TRUE),mean(data$w,na.rm = TRUE),mean(data$q,na.rm = TRUE)
,mean(data$ad,na.rm = TRUE),mean(data$per_y,na.rm = TRUE),mean(data$data_arc,na.rm = TRUE)
,mean(data$condition_code,na.rm = TRUE),mean(data$n_obs_used,na.rm = TRUE)
,mean(data$H,na.rm = TRUE),mean(data$diameter,na.rm = TRUE),mean(data$albedo,na.rm = TRUE)
,mean(data$rot_per,na.rm = TRUE),mean(data$moid,na.rm = TRUE))
#guardamos data con nulos:
data2=data
#Remplazar los nulos con la media.
for (i in 1:16){
variable=nombres[i]
data[[variable]]<- round(data[[variable]] %>%
replace(is.na(.),medias[i]), digits = 6)
}
#columnas con su respectivo numero de nulos
apply(X=is.na(data),MARGIN=2,FUN =sum)
#Correlaciones
muestra<-select_if(data,is.numeric)
#Menor a mayor considerando negativos:
cor(muestra, method="pearson")[13,][order(cor(muestra, method="pearson")[15,])]
#Menor a mayor en absoluto:
sort(abs(cor(muestra, method="pearson")[13,]))
sort(abs(cor(muestra, method="spearman")[13,]))
#Borramos data con correlacion bajo 0.2 en pearson y 0.3 en spearmann
data$w<-NULL
data$rot_per<-NULL
data$om<-NULL
data$e<-NULL
data$i<-NULL
data$albedo<-NULL
data$condition_code<-NULL
muestra<-select_if(data,is.numeric)
sort(abs(cor(muestra, method="pearson")[8,]))
sort(abs(cor(muestra, method="spearman")[8,]))
#Data hasta ahora considerando lo realizado con anterioridad.
nombres<-names(select_if(data, is.numeric))
nombres
for (i in 1:9){
variable=nombres[i]
hist(data[[variable]],main=nombres[i],col="red")
}
#Notamos que diametro tiene muchos valores extremos por lo que los eliminamos.
data<-data[data$diameter < 20, ]
for (i in 1:9){
variable=nombres[i]
hist(data[[variable]],main=nombres[i],col="blue")
}
#nulos por media, elimine categoricas y elimine correlaciones segun pearson y spearmann.
write.csv(data, file="data_nulos_media_sinoutliers.csv")
##Realizamos mismo tratamiento para test.####
#Eliminamos variables.
test$X<-NULL
test$index<-NULL
test$full_name<-NULL
test$G<-NULL
test$GM<-NULL
test$BV<-NULL
test$UB<-NULL
test$IR<-NULL
test$neo<-NULL
test$pha<-NULL
test$extent<-NULL
test$spec_B<-NULL
test$spec_T<-NULL
test$w<-NULL
test$rot_per<-NULL
test$om<-NULL
test$e<-NULL
test$i<-NULL
test$albedo<-NULL
test$condition_code<-NULL
nombres<-names(select_if(test, is.numeric))
nombres
medias<-c(mean(test$a,na.rm = TRUE),
mean(test$q,na.rm = TRUE),
mean(test$ad,na.rm = TRUE),
mean(test$per_y,na.rm = TRUE),
mean(test$data_arc,na.rm = TRUE),
mean(test$n_obs_used,na.rm = TRUE),
mean(test$H,na.rm = TRUE),
mean(test$moid,na.rm = TRUE),
)
pacman::p_load(psych,ggplot2,tidyverse,proxy,dplyr,rlang,purrr)
data<-read.csv("dataTrain.csv")
test<-read.csv("dataEval.csv")
## Viendo nulos de Eval: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(test),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos
#columnas con nulos:
Column_na<-sum(apply(X=is.na(test),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(test),MARGIN=2,FUN =sum)>0
a<-apply(X=is.na(test),MARGIN=2,FUN =sum)[valores_col]
a
Col_Eval_nulos<-names(a[a>37500])
Col_Eval_nulos
## Viendo nulos de Train: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(data),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos
#columnas con nulos:
Column_na<-sum(apply(X=is.na(data),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(data),MARGIN=2,FUN =sum)>0
b<-apply(X=is.na(data),MARGIN=2,FUN =sum)[valores_col]
b
Col_Train_nulos<-names(b[b>99500])
Col_Train_nulos
#Eliminamos los indices:
data$X<-NULL
data$index<-NULL
data$full_name<-NULL
#Elimnamos las columnas con mayor cantidad de nulos.
data$G<-NULL
data$GM<-NULL
data$BV<-NULL
data$UB<-NULL
data$IR<-NULL
#Borramos variables categoricas que no afectan a nuestro problema.
apply(X=is.na(data),MARGIN=2,FUN =sum)
data$neo<-NULL
data$pha<-NULL
data$extent<-NULL
data$spec_B<-NULL
data$spec_T<-NULL
#Siguen habiendo 99970 tuplas con nulos.
# Analisis Descriptivo ####
str(data)
nombres<-names(select_if(data, is.numeric))
nombres
medias<-c(mean(data$a,na.rm = TRUE),mean(data$e,na.rm = TRUE),mean(data$i,na.rm = TRUE)
,mean(data$om,na.rm = TRUE),mean(data$w,na.rm = TRUE),mean(data$q,na.rm = TRUE)
,mean(data$ad,na.rm = TRUE),mean(data$per_y,na.rm = TRUE),mean(data$data_arc,na.rm = TRUE)
,mean(data$condition_code,na.rm = TRUE),mean(data$n_obs_used,na.rm = TRUE)
,mean(data$H,na.rm = TRUE),mean(data$diameter,na.rm = TRUE),mean(data$albedo,na.rm = TRUE)
,mean(data$rot_per,na.rm = TRUE),mean(data$moid,na.rm = TRUE))
#guardamos data con nulos:
data2=data
#Remplazar los nulos con la media.
for (i in 1:16){
variable=nombres[i]
data[[variable]]<- round(data[[variable]] %>%
replace(is.na(.),medias[i]), digits = 6)
}
#columnas con su respectivo numero de nulos
apply(X=is.na(data),MARGIN=2,FUN =sum)
#Correlaciones
muestra<-select_if(data,is.numeric)
#Menor a mayor considerando negativos:
cor(muestra, method="pearson")[13,][order(cor(muestra, method="pearson")[15,])]
#Menor a mayor en absoluto:
sort(abs(cor(muestra, method="pearson")[13,]))
sort(abs(cor(muestra, method="spearman")[13,]))
#Borramos data con correlacion bajo 0.2 en pearson y 0.3 en spearmann
data$w<-NULL
data$rot_per<-NULL
data$om<-NULL
data$e<-NULL
data$i<-NULL
data$albedo<-NULL
data$condition_code<-NULL
muestra<-select_if(data,is.numeric)
sort(abs(cor(muestra, method="pearson")[8,]))
sort(abs(cor(muestra, method="spearman")[8,]))
#Data hasta ahora considerando lo realizado con anterioridad.
nombres<-names(select_if(data, is.numeric))
nombres
for (i in 1:9){
variable=nombres[i]
hist(data[[variable]],main=nombres[i],col="red")
}
#Notamos que diametro tiene muchos valores extremos por lo que los eliminamos.
data<-data[data$diameter < 20, ]
for (i in 1:9){
variable=nombres[i]
hist(data[[variable]],main=nombres[i],col="blue")
}
#nulos por media, elimine categoricas y elimine correlaciones segun pearson y spearmann.
write.csv(data, file="data_nulos_media_sinoutliers.csv")
##Realizamos mismo tratamiento para test.####
#Eliminamos variables.
test$X<-NULL
test$index<-NULL
test$full_name<-NULL
test$G<-NULL
test$GM<-NULL
test$BV<-NULL
test$UB<-NULL
test$IR<-NULL
test$neo<-NULL
test$pha<-NULL
test$extent<-NULL
test$spec_B<-NULL
test$spec_T<-NULL
test$w<-NULL
test$rot_per<-NULL
test$om<-NULL
test$e<-NULL
test$i<-NULL
test$albedo<-NULL
test$condition_code<-NULL
nombres<-names(select_if(test, is.numeric))
nombres
medias<-c(mean(test$a,na.rm = TRUE),
mean(test$q,na.rm = TRUE),
mean(test$ad,na.rm = TRUE),
mean(test$per_y,na.rm = TRUE),
mean(test$data_arc,na.rm = TRUE),
mean(test$n_obs_used,na.rm = TRUE),
mean(test$H,na.rm = TRUE),
mean(test$moid,na.rm = TRUE),
)
pacman::p_load(psych,ggplot2,tidyverse,proxy,dplyr,rlang,purrr)
data<-read.csv("dataTrain.csv")
test<-read.csv("dataEval.csv")
## Viendo nulos de Eval: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(test),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos
#columnas con nulos:
Column_na<-sum(apply(X=is.na(test),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(test),MARGIN=2,FUN =sum)>0
a<-apply(X=is.na(test),MARGIN=2,FUN =sum)[valores_col]
a
Col_Eval_nulos<-names(a[a>37500])
Col_Eval_nulos
## Viendo nulos de Train: ####
#Tuplas con nulos:
Tuplas_na<-sum(apply(X=is.na(data),MARGIN=1,FUN =sum)>0)
Tuplas_na #Todas las tuplas tienen nulos
#columnas con nulos:
Column_na<-sum(apply(X=is.na(data),MARGIN=2,FUN =sum)>0)
#columnas con su respectivo numero de nulos
valores_col<-apply(X=is.na(data),MARGIN=2,FUN =sum)>0
b<-apply(X=is.na(data),MARGIN=2,FUN =sum)[valores_col]
b
Col_Train_nulos<-names(b[b>99500])
Col_Train_nulos
#Eliminamos los indices:
data$X<-NULL
data$index<-NULL
data$full_name<-NULL
#Elimnamos las columnas con mayor cantidad de nulos.
data$G<-NULL
data$GM<-NULL
data$BV<-NULL
data$UB<-NULL
data$IR<-NULL
#Borramos variables categoricas que no afectan a nuestro problema.
apply(X=is.na(data),MARGIN=2,FUN =sum)
data$neo<-NULL
data$pha<-NULL
data$extent<-NULL
data$spec_B<-NULL
data$spec_T<-NULL
#Siguen habiendo 99970 tuplas con nulos.
# Analisis Descriptivo ####
str(data)
nombres<-names(select_if(data, is.numeric))
nombres
medias<-c(mean(data$a,na.rm = TRUE),mean(data$e,na.rm = TRUE),mean(data$i,na.rm = TRUE)
,mean(data$om,na.rm = TRUE),mean(data$w,na.rm = TRUE),mean(data$q,na.rm = TRUE)
,mean(data$ad,na.rm = TRUE),mean(data$per_y,na.rm = TRUE),mean(data$data_arc,na.rm = TRUE)
,mean(data$condition_code,na.rm = TRUE),mean(data$n_obs_used,na.rm = TRUE)
,mean(data$H,na.rm = TRUE),mean(data$diameter,na.rm = TRUE),mean(data$albedo,na.rm = TRUE)
,mean(data$rot_per,na.rm = TRUE),mean(data$moid,na.rm = TRUE))
#guardamos data con nulos:
data2=data
#Remplazar los nulos con la media.
for (i in 1:16){
variable=nombres[i]
data[[variable]]<- round(data[[variable]] %>%
replace(is.na(.),medias[i]), digits = 6)
}
#columnas con su respectivo numero de nulos
apply(X=is.na(data),MARGIN=2,FUN =sum)
#Correlaciones
muestra<-select_if(data,is.numeric)
#Menor a mayor considerando negativos:
cor(muestra, method="pearson")[13,][order(cor(muestra, method="pearson")[15,])]
#Menor a mayor en absoluto:
sort(abs(cor(muestra, method="pearson")[13,]))
sort(abs(cor(muestra, method="spearman")[13,]))
#Borramos data con correlacion bajo 0.2 en pearson y 0.3 en spearmann
data$w<-NULL
data$rot_per<-NULL
data$om<-NULL
data$e<-NULL
data$i<-NULL
data$albedo<-NULL
data$condition_code<-NULL
muestra<-select_if(data,is.numeric)
sort(abs(cor(muestra, method="pearson")[8,]))
sort(abs(cor(muestra, method="spearman")[8,]))
#Data hasta ahora considerando lo realizado con anterioridad.
nombres<-names(select_if(data, is.numeric))
nombres
for (i in 1:9){
variable=nombres[i]
hist(data[[variable]],main=nombres[i],col="red")
}
#Notamos que diametro tiene muchos valores extremos por lo que los eliminamos.
data<-data[data$diameter < 20, ]
for (i in 1:9){
variable=nombres[i]
hist(data[[variable]],main=nombres[i],col="blue")
}
#nulos por media, elimine categoricas y elimine correlaciones segun pearson y spearmann.
write.csv(data, file="data_nulos_media_sinoutliers.csv")
##Realizamos mismo tratamiento para test.####
#Eliminamos variables.
test$X<-NULL
test$index<-NULL
test$full_name<-NULL
test$G<-NULL
test$GM<-NULL
test$BV<-NULL
test$UB<-NULL
test$IR<-NULL
test$neo<-NULL
test$pha<-NULL
test$extent<-NULL
test$spec_B<-NULL
test$spec_T<-NULL
test$w<-NULL
test$rot_per<-NULL
test$om<-NULL
test$e<-NULL
test$i<-NULL
test$albedo<-NULL
test$condition_code<-NULL
nombres<-names(select_if(test, is.numeric))
nombres
medias<-c(mean(test$a,na.rm = TRUE),
mean(test$q,na.rm = TRUE),
mean(test$ad,na.rm = TRUE),
mean(test$per_y,na.rm = TRUE),
mean(test$data_arc,na.rm = TRUE),
mean(test$n_obs_used,na.rm = TRUE),
mean(test$H,na.rm = TRUE),
mean(test$moid,na.rm = TRUE)
)
#Remplazar los nulos con la media.
for (i in 1:8){
variable=nombres[i]
test[[variable]]<- round(test[[variable]] %>%
replace(is.na(.),medias[i]), digits = 6)
}
write.csv(test, file="eval_nulosmedia.csv")
